[
  {
    "title": "Qwen3",
    "description": "Qwen3 represents the latest generation in the Qwen family of large language models, featuring both MoE (Mixture-of-Experts) and dense models. The flagship model, Qwen3-235B-A22B, has 235 billion total parameters with 22 billion activated parameters, achieving competitive results against top models like DeepSeek-R1, o1, o3-mini, and others.\n\nKey features:\n- **Hybrid Thinking Modes**: Supports both a step-by-step reasoning mode for complex problems and a quick response mode for simpler questions\n- **Extensive Multilingual Support**: Capable of handling 119 languages and dialects\n- **Enhanced Agentic Capabilities**: Optimized for coding tasks with improved Model Context Protocol (MCP) support\n- **Improved Performance**: Outperforms larger models while using fewer parameters\n\nThe release includes two MoE models (Qwen3-235B-A22B and Qwen3-30B-A3B) and six dense models (Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B), all available under Apache 2.0 license. The models feature context lengths ranging from 32K to 128K tokens and can be deployed using frameworks like SGLang, vLLM, Ollama, and LMStudio.\n\nQwen3 was pre-trained on approximately 36 trillion tokens (twice the data of Qwen2.5) across a three-stage process, with increased focus on high-quality data for STEM, coding, and reasoning tasks. The post-training pipeline includes long chain-of-thought cold start, reasoning-based reinforcement learning, thinking mode fusion, and general reinforcement learning.",
    "launch_date": "2025-04-29",
    "launch_url": "https://qwenlm.github.io/blog/qwen3/",
    "tags": ["research", "oss"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/QwenLM/Qwen3",
    "company_url": "https://qwenlm.github.io",
    "authors": [],
    "image_url": null,
    "slug": "qwen3-2025-04-29"
  },
  {
    "title": "OpenAI API Agents",
    "description": "OpenAI has introduced new tools for building AI agents through their platform, designed to help developers and enterprises create useful and reliable agentic systems. The API Agents offering appears to be focused on simplifying agent development with features that likely include agent coordination, task management, and enhanced reliability. The platform evolves OpenAI's capabilities beyond model access to provide a more comprehensive framework for building production-ready AI agents that can autonomously perform tasks while maintaining appropriate guardrails. While specific technical details are limited in the announcement, the release represents OpenAI's entry into the growing space of agent development frameworks that enable developers to build, deploy, and manage complex AI agent systems.",
    "launch_date": "2025-03-11",
    "launch_url": "https://openai.com/news/new-tools-for-building-agents",
    "tags": ["product", "framework", "agent"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.openai.com",
    "authors": [],
    "image_url": null,
    "slug": "openai-api-agents-2025-03-11"
  },
  {
    "title": "Agent Development Kit (ADK)",
    "description": "Agent Development Kit (ADK) is an open-source framework from Google designed to simplify the full stack end-to-end development of agents and multi-agent systems. It empowers developers to build production-ready agentic applications with greater flexibility and precise control.\n\nKey features:\n- **Multi-Agent by Design**: Build modular and scalable applications by composing multiple specialized agents in a hierarchy with complex coordination and delegation\n- **Rich Model Ecosystem**: Supports various models via Vertex AI Model Garden and LiteLLM integration, allowing flexibility across providers\n- **Rich Tool Ecosystem**: Pre-built tools (Search, Code Exec), Model Context Protocol (MCP) tools, 3rd-party library integration (LangChain, LlamaIndex), and agent-as-tool capability\n- **Built-in Streaming**: Bidirectional audio and video streaming for natural interactions\n- **Flexible Orchestration**: Define workflows using workflow agents for predictable pipelines or leverage LLM-driven dynamic routing\n- **Integrated Developer Experience**: Develop, test, and debug locally with CLI and visual Web UI\n- **Built-in Evaluation**: Systematically assess agent performance against predefined test cases\n- **Easy Deployment**: Containerize and deploy agents anywhere, with optimized integration for Google Cloud\n\nADK is the same framework powering agents within Google products like Agentspace and the Google Customer Engagement Suite (CES), now made available as open-source to provide developers with flexible tools for the rapidly evolving agent landscape.",
    "launch_date": "2025-04-09",
    "launch_url": "https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/",
    "tags": ["oss", "framework", "agent"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://google.github.io/adk-docs/",
    "company_url": "https://www.google.com",
    "authors": ["Erwin Huizenga", "Bo Yang"],
    "image_url": null,
    "slug": "agent-development-kit-adk-2025-04-09"
  },
  {
    "title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories",
    "description": "PlanGEN is a model-agnostic, scalable multi-agent framework for enhancing large language models' ability to generate effective natural language plans and solve complex reasoning problems. The framework consists of three specialized agents: a constraint agent that extracts instance-specific constraints, a verification agent that evaluates plan quality using those constraints, and a selection agent that dynamically chooses the most suitable inference algorithm based on instance complexity.\n\nKey innovations:\n- **Constraint-Guided Verification**: Extracts problem-specific constraints that guide verification of generated plans\n- **Adaptive Selection**: Uses a modified UCB (Upper Confidence Bound) policy to select the most suitable inference algorithm based on instance complexity\n- **Multiple Inference Algorithms**: Integrates with Best of N, Tree-of-Thought, and REward-BAlanced SEarch (REBASE)\n- **Mixture Approach**: Dynamically selects the optimal algorithm for each problem instance\n\nThe framework achieves state-of-the-art results across multiple benchmarks, including NATURAL PLAN (~8% improvement), OlympiadBench (~4% improvement on MATH, ~4% on PHYSICS), DocFinQA (~7% improvement), and GPQA (~1% improvement). Experiments demonstrate that simpler methods like PlanGEN (Best of N) excel at straightforward planning tasks, while PlanGEN (Mixture of Algorithms) performs best on more complex reasoning problems.",
    "launch_date": "2025-02-22",
    "launch_url": "https://arxiv.org/abs/2502.16111",
    "tags": ["research", "agent"],
    "paper_pdf_url": "https://arxiv.org/pdf/2502.16111.pdf",
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.google.com",
    "authors": [
      "Mihir Parmar",
      "Xin Liu",
      "Palash Goyal",
      "Yanfei Chen",
      "Long Le",
      "Swaroop Mishra",
      "Hossein Mobahi",
      "Jindong Gu",
      "Zifeng Wang",
      "Hootan Nakhost",
      "Chitta Baral",
      "Chen-Yu Lee",
      "Tomas Pfister",
      "Hamid Palangi"
    ],
    "image_url": null,
    "slug": "plangen-multi-agent-framework-planning-reasoning-2025-02-22"
  },
  {
    "title": "Phi-4 Multimodal & Phi-4 Mini",
    "description": "Microsoft introduces two new models in its Phi family of small language models (SLMs): Phi-4-multimodal, their first multimodal language model, and Phi-4-mini, a compact text-focused model optimized for efficiency.\n\n### Core Features\n**Phi-4-multimodal (5.6B parameters)**\n- First multimodal model in the Phi family\n- Seamless integration of speech, vision, and text modalities\n- Mixture-of-LoRAs architecture for unified processing\n- Optimized for on-device and edge computing\n\n**Phi-4-mini (3.8B parameters)**\n- Dense, decoder-only transformer architecture\n- Grouped-query attention mechanism\n- 200,000 vocabulary with shared input-output embeddings\n- Support for sequences up to 128,000 tokens\n\n### Technical Performance\n**Phi-4-multimodal**\n- Top position on Huggingface OpenASR leaderboard (6.14% word error rate)\n- Outperforms specialized models like WhisperV3 in speech recognition\n- Strong performance in document understanding, chart analysis, and OCR\n- Competitive results in visual science reasoning tasks\n\n**Phi-4-mini**\n- Excels at reasoning, math, coding, and instruction-following\n- Strong function-calling capabilities despite compact size\n- Outperforms larger models on various text-based benchmarks\n\n### Implementation Support\n- Both models available in Azure AI Foundry and HuggingFace\n- Phi-4-multimodal also available in NVIDIA API Catalog\n- Cross-platform deployment through ONNX Runtime optimization\n- Rapid customization with significant performance gains on domain-specific tasks\n\n### Agentic Capabilities\n- Enable efficient on-device reasoning and multimodal perception\n- Support for embedded devices and edge computing scenarios\n- Function calling enables integration with external tools and APIs\n- Facilitate development of lightweight autonomous systems with real-time environmental awareness\n- Particularly valuable for automotive, healthcare, financial services, and smart device applications",
    "launch_date": "2025-02-25",
    "launch_url": "https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/",
    "tags": ["research", "product"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/microsoft/PhiCookBook",
    "company_url": "https://www.microsoft.com",
    "authors": ["Microsoft Research Team"],
    "image_url": null,
    "slug": "phi-4-multimodal-mini-2025-02-25"
  },
  {
    "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
    "description": "UI-TARS is a next-generation native GUI agent model that solely perceives screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks.\n\n### Core Capabilities\n\n**Perception**:\n- Comprehensive GUI Understanding: Processes multimodal inputs for coherent interface interpretation\n- Real-Time Interaction: Continuously monitors dynamic GUIs and responds accurately\n- Enhanced Visual Perception: Leverages large-scale GUI screenshot datasets\n\n**Action**:\n- Unified Action Space: Standardized actions across platforms (desktop, mobile, web)\n- Platform-Specific Actions: Supports hotkeys, long press, and platform-specific gestures\n- High-Precision Grounding: Accurate coordinate prediction for interactions\n\n**Reasoning**:\n- System 1 & System 2 Thinking: Combines fast, intuitive responses with deliberate planning\n- Task Decomposition & Reflection: Supports multi-step planning and error correction\n\n**Memory**:\n- Short-Term Memory: Captures task-specific context\n- Long-Term Memory: Retains historical interactions and knowledge\n\n### Performance Highlights\n\n- **OSWorld**: Achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude's 22.0 and 14.9 respectively\n- **AndroidWorld**: Achieves 46.6, surpassing GPT-4o's 34.5\n\n### Implementation & Deployment\n\n- **Cloud Deployment**: Available via HuggingFace Inference Endpoints\n- **Local Deployment**: Supports vLLM (≥0.6.1) and Transformers\n- **Model Variants**: Available in 2B, 7B, and 72B parameter sizes\n\n### Related Agent Frameworks\n\nFor developers interested in building multiagent systems, UI-TARS can be integrated with frameworks like:\n\n- [AutoGen](https://github.com/microsoft/autogen): Open-source framework for autonomous agents\n- [CrewAI](https://github.com/crewAIInc/crewAI): Framework for role-playing autonomous agents\n- [LangGraph](https://github.com/langchain-ai/langgraph): Library for stateful multi-actor applications\n\nThe model and additional resources are available on [GitHub](https://github.com/bytedance/UI-TARS).",
    "launch_date": "2025-01-21",
    "launch_url": "https://github.com/bytedance/UI-TARS",
    "tags": ["research", "product", "oss", "agent"],
    "paper_pdf_url": "https://arxiv.org/pdf/2501.12326",
    "tweet_url": null,
    "github_url": "https://github.com/bytedance/UI-TARS",
    "company_url": "https://www.bytedance.com",
    "authors": [
      "Yujia Qin",
      "Yining Ye",
      "Junjie Fang",
      "Haoming Wang",
      "Shihao Liang",
      "Shizuo Tian",
      "Junda Zhang",
      "Jiahao Li",
      "Yunxin Li",
      "Shijue Huang",
      "et al."
    ],
    "image_url": null,
    "slug": "ui-tars-gui-agent-2024-01-21"
  },
  {
    "title": "DeepSeek R1",
    "description": "DeepSeek-R1 introduces large-scale reinforcement learning (RL) for enhancing model reasoning capabilities without extensive supervised data.\n\n### Core Features\n- First model trained via pure RL without initial supervised fine-tuning (R1-Zero variant)\n- Multi-stage RL and cold-start training pipeline for R1\n- Knowledge distillation to smaller dense models\n\n### Technical Performance\n- AIME 2024: 79.8% Pass@1\n- MATH-500: 97.3% Pass@1\n- MMLU: 90.8% accuracy\n- GPQA Diamond: 71.5% Pass@1\n- SWE-Bench Verified: 49.2% resolved\n- LiveCodeBench: 65.9% Pass@1\n\n### Distilled Models Released\n- Dense models from 1.5B to 70B parameters\n- Based on Qwen and Llama architectures\n- 32B & 70B variants achieve performance close to OpenAI-o1-mini\n\n### API Pricing\n- Input (cache hit): $0.14/M tokens\n- Input (cache miss): $0.55/M tokens  \n- Output: $2.19/M tokens",
    "launch_date": "2025-01-20",
    "launch_url": "https://github.com/deepseek-ai/DeepSeek-R1",
    "tags": ["research", "oss"],
    "paper_pdf_url": "https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf",
    "tweet_url": null,
    "github_url": "https://github.com/deepseek-ai/DeepSeek-R1",
    "company_url": "https://www.deepseek.com",
    "authors": ["DeepSeek-AI Team"],
    "image_url": null,
    "slug": "deepseek-r1-2025-01-20"
  },
  {
    "title": "Phi-4",
    "description": "Microsoft has released Phi-4, a 14B parameter small language model (SLM) focused on complex reasoning and mathematical tasks.\n\n### Core Features\n- 14B total parameters\n- Strong performance on math and reasoning tasks\n- Optimized for efficiency and small model size\n- Available on Azure AI Foundry and Hugging Face\n\n### Technical Performance\n- AMC 12 (math competition problems): 54.5%\n- GSM8K: 84.3%\n- HumanEval: 67.8%\n- MMLU: 79.4%\n\n### Implementation Support\n- Fully compatible with Azure AI Foundry\n- Content safety features including:\n  - Prompt shields\n  - Protected material detection\n  - Groundedness detection\n- Real-time monitoring capabilities\n\n### Current Limitations\n- Size constraints compared to larger models\n- Limited multimodal capabilities\n- Task-specific optimization may impact general performance",
    "launch_date": "2024-12-13",
    "launch_url": "https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/introducing-phi-4-microsoft-s-newest-small-language-model/ba-p/4357090",
    "tags": ["research", "product"],
    "paper_pdf_url": "https://aka.ms/Phi-4TechReport",
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.microsoft.com",
    "authors": ["Microsoft Research Team"],
    "image_url": null,
    "slug": "phi-4-2024-12-13"
  },
  {
    "title": "DeepSeek V3",
    "description": "DeepSeek announces V3 of their open-source model, featuring a MoE (Mixture-of-Experts) architecture with 671B total parameters and 37B activated parameters per token.\n\n### Core Features\n- 3x faster than V2 (60 tokens/second claimed)\n- Multi-head Latent Attention (MLA) for efficient inference\n- 128K context window\n- Pre-trained on 14.8T tokens\n\n### Technical Performance\n- HumanEval: 65.2% Pass@1\n- MATH: 61.6% accuracy\n- MMLU: 88.5% accuracy\n- AGIEval: 79.6% accuracy\n- SWE-Bench Verified: 42.0% resolved\n- LiveCodeBench: 40.5% Pass@1\n\n### Implementation Support\n- Compatible with: SGLang, LMDeploy, TensorRT-LLM, vLLM\n- Hardware support: NVIDIA GPUs, AMD GPUs, Huawei Ascend NPUs\n\n### Pricing (from Feb 8, 2024)\n- Input (cache miss): $0.27/M tokens\n- Input (cache hit): $0.07/M tokens\n- Output: $1.10/M tokens",
    "launch_date": "2024-12-26",
    "launch_url": "https://github.com/deepseek-ai/DeepSeek-V3",
    "tags": ["research", "oss"],
    "paper_pdf_url": "https://arxiv.org/pdf/2412.19437",
    "tweet_url": null,
    "github_url": "https://github.com/deepseek-ai/DeepSeek-V3",
    "company_url": "https://www.deepseek.com",
    "authors": ["DeepSeek-AI Team"],
    "image_url": null,
    "slug": "deepseek-v3-2024-12-26"
  },
  {
    "title": "SWE-bench Verified",
    "description": "**SWE-bench Verified** is a human-validated subset of SWE-bench, designed to more reliably evaluate AI models' ability to solve real-world software issues. The dataset consists of 500 samples verified by professional software developers to be non-problematic, addressing three major improvements over the original benchmark:\n\n1. **Test Quality**: Ensures unit tests accurately evaluate solution correctness without rejecting valid solutions\n2. **Problem Clarity**: Validates that issue descriptions are well-specified and unambiguous\n3. **Environment Reliability**: Improves development environment setup reliability\n\n**Key Features**:\n- 500 carefully curated samples from the original SWE-bench dataset\n- Comprehensive human annotations from 93 experienced Python developers\n- New Docker-based evaluation harness for improved reliability\n- Complete annotation dataset and rubric available publicly\n\n**Performance Impact**:\n- GPT-4o achieves 33.2% success rate (vs 16% on original SWE-bench)\n- Agentless scaffold doubled performance to 32.8%\n- Significant improvements across all major scaffolds\n\n**Methodology**:\n- Each sample annotated by 3 separate annotators\n- Conservative ensembling using highest-severity labels\n- Detailed annotation rubric covering issue specification and test validity\n- Samples filtered based on severity ratings and additional quality criteria\n\nThis release represents a significant improvement in benchmark reliability for evaluating autonomous software engineering capabilities, providing a more accurate assessment of AI models' abilities to solve real-world programming issues.",
    "launch_date": "2024-08-13",
    "launch_url": "https://www.openai.com/blog/introducing-swe-bench-verified",
    "tags": ["research", "benchmark", "oss"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified",
    "company_url": "https://www.openai.com",
    "authors": [
      "Neil Chowdhury",
      "James Aung",
      "Chan Jun Shern",
      "Oliver Jaffe",
      "Dane Sherburn",
      "Giulio Starace",
      "Evan Mays",
      "Rachel Dias",
      "Marwan Aljubeh",
      "Mia Glaese",
      "Carlos E. Jimenez",
      "John Yang",
      "Kevin Liu",
      "Aleksander Madry"
    ],
    "image_url": null,
    "slug": "swe-bench-verified-2024-08-13"
  },

  {
    "title": "Veo 2",
    "description": "Google DeepMind introduces Veo 2, a video generation model with expanded capabilities over its predecessor. \n\n### Core Features\n- Output resolution up to 4K\n- Camera motion control including tracking shots, pans, and zooms\n- Multiple resolution modes for video output\n- Advanced motion simulation for natural movement\n- Reduced visual artifacts compared to prior models\n\n### Technical Performance\n- Human evaluations showed superior performance on the MovieGenBench dataset\n  - 1003 prompts and videos evaluated\n  - Ranked highest in prompt following accuracy\n  - Ranked highest in overall preference vs competing models\n  - Videos evaluated at 720p baseline resolution\n  - 8-second sample duration (compared to 5s for most models and 10s for VideoGen)\n\n### Applications\n- Demonstrated capabilities across:\n  - Character animation and movement\n  - Natural scenes and environments\n  - Abstract and stylized content\n  - Complex camera movements\n  - Varying artistic styles\n\n### Current Limitations\n- Maintaining complete consistency throughout complex scenes\n- Creating intricate motion sequences\n- Precise control in dynamic scenes\n- Full reliability in complex scene generation\n\n### Availability\nAccess to Veo 2 will be provided through Google's VideoFX platform.",
    "launch_date": "2024-12-16",
    "launch_url": "https://blog.google/technology/google-labs/video-image-generation-update-december-2024",
    "tags": ["research", "product"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://deepmind.google/",
    "authors": ["Google DeepMind Research Team"],
    "image_url": null,
    "slug": "veo-2-2024-12-16"
  },
  {
    "title": "Gemini 2.0: Google's Foundation Model for the Agentic Era",
    "description": "**Gemini 2.0** represents Google DeepMind's next generation foundation model designed specifically for powering AI agents. The first release, **Gemini 2.0 Flash**, introduces core agentic capabilities including:\n\n- **Native Agent Capabilities**:\n  - Autonomous task planning and execution\n  - Native tool use including Google Search integration\n  - Real-time decision making and action generation\n  - Complex instruction following with safety constraints\n\n- **Multimodal Understanding & Generation**:\n  - Image and audio output generation\n  - Multilingual text-to-speech\n  - Real-time video understanding\n  - Long context processing\n\n- **Agent Applications**:\n  - **Project Mariner**: Web navigation agent achieving 83.5% on WebVoyager benchmark\n  - **Jules**: AI coding agent integrated with GitHub workflows\n  - **Project Astra**: Universal AI assistant with 10-minute conversation memory\n  - **Gaming Agents**: Real-time game understanding and strategy assistance\n\nGemini 2.0 Flash is available via:\n- Gemini API (Google AI Studio/Vertex AI)\n- Gemini app chat interface\n- Early access partnerships for advanced features\n\nThe release includes a new **Multimodal Live API** supporting real-time agent interactions through audio/video streaming and multi-tool integration. Google emphasizes responsible development with extensive safety measures including AI-assisted red teaming and automated risk mitigation.",
    "launch_date": "2024-12-11",
    "launch_url": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/",
    "tags": ["research", "product", "agent"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://deepmind.google/",
    "authors": ["Sundar Pichai", "Demis Hassabis", "Koray Kavukcuoglu"],
    "image_url": null,
    "slug": "gemini-2-0-agent-foundation-model-2024-12-11"
  },
  {
    "title": "Kura AI: Browser Agent System",
    "description": "Kura AI introduces a browser agent system for automating web interactions. \n\nKey claimed features:\n- **Performance**: 87% accuracy on WebVoyager benchmark, stated to be 31% better than Claude's Computer Use demo and 14% above previous state-of-the-art\n- **Architecture**: Multi-agent system using planner, executor, and critic in a debate framework\n- **Flexibility**: Model-agnostic design potentially allowing up to 90% cost reduction vs Claude's Computer Use\n- **Integration**: Converts UI interactions into API-ready endpoints for agentic applications\n\nThe system uses a combination of computer vision and DOM analysis, with agents having varying degrees of visual and HTML context access. The architecture aims to enable self-healing and backtracking capabilities for handling complex web interactions.\n\nKura positions their system as a bridge between human-centric web interfaces and AI agents, focusing on production-ready implementations for enterprise automation.",
    "launch_date": "2024-11-15",
    "launch_url": "https://www.ycombinator.com/launches/MGd-kura-new-state-of-the-art-for-browser-agents-31-better-than-computer-use",
    "tags": ["startup", "product"],
    "paper_pdf_url": null,
    "tweet_url": "https://x.com/ycombinator/status/1857470232312647746",
    "github_url": null,
    "company_url": "https://trykura.com",
    "authors": ["Darren Hwang", "Ronit Basu"],
    "image_url": null,
    "slug": "kura-ai-browser-agent-2024-11-15"
  },
  {
    "title": "Runner H: Production-Ready Web Automation Agent",
    "description": "**H Company** introduces the **Studio**, a platform for developers to effortlessly create production-ready and robust automations at scale. Their flagship agent, **Runner H**, available in private beta, enables natural language-driven web automation with state-of-the-art performance.\n\nKey claimed features:\n- Outperforms Anthropic Computer Use on the [WebVoyager benchmark](https://hcompany.ai/blog/a-research-update)\n- Automated design of web automation pipelines\n- Self-healing capabilities for adapting to UI changes\n- Specialized foundation models optimized for cost-efficiency\n\nThe **Studio platform** includes:\n- Cloud API for managed agents\n- Interface for creating automations\n- Tools for reviewing and editing past/live runs\n- Comprehensive automation monitoring\n\n**Technical achievements**:\n- Best-in-class Vision Language Model (VLM) for UI interaction\n- Top performance on Screenspot benchmark for UI Action Models\n- Superior web agent capabilities for open-ended tasks\n- Efficient, specialized models outperforming larger generalist approaches\n\n**Use cases**:\n- End-to-end e-commerce automation (product discovery to order confirmation)\n- Financial services onboarding\n- Multi-step verification processes\n- Document handling and compliance checks\n\nThe roadmap includes:\n- Advanced reinforcement learning and distillation techniques\n- Enhanced debugging and teaching capabilities\n- Developer community support\n- Enterprise-grade security standards\n\nH Company aims to democratize agents, allowing developers and teams to focus on meaningful work by automating repetitive tasks. The approach here appears similar to work done [Adept.ai](https://www.adept.ai/) over the last 2 years. It will be interesting to see how they advance the state of the art here.",
    "launch_date": "2024-11-19",
    "launch_url": "https://hcompany.ai/blog/introducing-h",
    "tags": ["product", "startup"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://hcompany.ai",
    "authors": ["H Team"],
    "image_url": null,
    "slug": "runner-h-production-web-automation-2024-11-19"
  },
  {
    "title": "Magentic-One: A Generalist Multi-Agent System",
    "description": "Microsoft Research has released Magentic-One, a multi-agent system for solving complex generalist tasks. The system features a novel architecture with an Orchestrator agent that coordinates four specialized agents through structured planning and progress tracking.\n\nKey innovations:\n- **Novel Architecture**: Orchestrator-led team of 5 specialized agents\n- **Dual Ledger System**: Task Ledger for facts/planning and Progress Ledger for execution tracking\n- **Adaptive Planning**: Built-in replanning after detecting stalls (2 retries)\n- **Specialized Agents**:\n  - FileSurfer: Handles diverse file types\n  - WebSurfer: Manages browser interactions\n  - Coder: Writes programs\n  - ComputerTerminal: Executes code\n\nPerformance metrics:\n- GAIA: 38% task completion\n- AssistantBench: 27.7% accuracy\n- WebArena: 32.8% success rate\n\nThe system achieves statistically comparable results to specialized state-of-the-art systems across multiple benchmarks while maintaining a generalist approach. Built on Microsoft's AutoGen framework, Magentic-One demonstrates the effectiveness of structured multi-agent coordination for complex task completion.",
    "launch_date": "2024-11-04",
    "launch_url": "https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/",
    "tags": ["research", "oss"],
    "paper_pdf_url": "https://aka.ms/magentic-one-report",
    "tweet_url": null,
    "github_url": "https://aka.ms/magentic-one",
    "company_url": "https://www.microsoft.com/en-us/research/",
    "authors": [
      "Adam Fourney",
      "Gagan Bansal",
      "Hussein Mozannar",
      "Cheng Tan",
      "Eduardo Salinas",
      "Erkang (Eric) Zhu",
      "Friederike Niedtner",
      "Grace Proebsting",
      "Griffin Bassman",
      "Jack Gerrits",
      "Jacob Alber",
      "Peter Chang",
      "Ricky Loynd",
      "Robert West",
      "Victor Dibia",
      "Ahmed Awadallah",
      "Ece Kamar",
      "Rafah Hosn",
      "Saleema Amershi"
    ],
    "image_url": null,
    "slug": "magentic-one-generalist-multi-agent-system-2024-11-04"
  },
  {
    "title": "π0: A Generalist Robot Policy for Physical Intelligence",
    "description": "**Physical Intelligence** has developed **π0** (pi-zero), a breakthrough general-purpose robot foundation model trained on the largest robot interaction dataset to date. The model takes natural language instructions and visual input to control multiple types of robots, outputting direct motor commands at up to 50 times per second for precise physical manipulation. At its core, π0 combines a 3 billion parameter vision-language model (VLM) pre-trained on internet-scale data with a novel flow matching technique to generate continuous action outputs. This architecture allows the model to transfer semantic understanding from internet pre-training to physical tasks, while maintaining high-frequency robot control. The system can be deployed either through zero-shot instruction following or fine-tuned for specialized complex tasks.\n\nKey innovations:\n- **Novel Architecture**: Built on a 3B parameter vision-language model (VLM), augmented with continuous action outputs via flow matching\n- **High-Frequency Control**: Capable of outputting motor commands up to 50 times per second\n- **Multi-Modal Integration**: Spans images, text, and actions while acquiring physical intelligence from embodied robot experience\n\nTraining approach:\n- Pre-trained on Internet-scale vision-language data\n- Trained on diverse datasets including:\n  - Open-source robot manipulation datasets\n  - Custom dexterous task collection across 8 distinct robots\n  - Large-scale multi-task and multi-robot interactions\n\nDemonstrated capabilities:\n- **Complex Task Execution**:\n  - Autonomous laundry folding from unstructured piles\n  - Intelligent table bussing with object sorting\n  - Cardboard box assembly requiring complex manipulation\n  - Adaptive strategies for handling various objects\n  - Recovery from human interventions\n\n**Performance Metrics**:\n- Significantly outperformed existing models in zero-shot evaluation\n- Full π0 showed >2x improvement over π0-small (470M parameter version)\n- Surpassed both OpenVLA (7B parameters) and Octo (93M parameters) across test tasks\n\n**Deployment Options**:\n- Zero-shot prompting for immediate task execution\n- Fine-tuning capabilities for specialized applications\n- Cross-platform compatibility with various robot systems\n\nThis release represents a significant step toward general-purpose robot foundation models that can follow natural language instructions, adapt to diverse environments, and execute complex physical tasks. The Physical Intelligence team is actively seeking collaborations, particularly with companies scaling up data collection with robots deployed for real-world applications.",
    "launch_date": "2024-10-31",
    "launch_url": "https://physicalintelligence.company/blog/pi0",
    "tags": ["research", "product"],
    "paper_pdf_url": "https://physicalintelligence.company/download/pi0.pdf",
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://physicalintelligence.company",
    "authors": [
      "Kevin Black",
      "Noah Brown",
      "Danny Driess",
      "Michael Equi",
      "Adnan Esmail",
      "Chelsea Finn",
      "Nick Fusai",
      "Lachy Groom",
      "Karol Hausman",
      "Brian Ichter",
      "Szymon Jakubczak",
      "Tim Jones",
      "Kay Ke",
      "Sergey Levine",
      "Adrian Li-Bell",
      "Mohith Mothukuri",
      "Suraj Nair",
      "Karl Pertsch",
      "Lucy Shi",
      "James Tanner",
      "Quan Vuong",
      "Anna Walling",
      "Haohuan Wang",
      "Ury Zhilinsky"
    ],
    "image_url": null,
    "slug": "pi0-generalist-robot-policy-2024-10-31"
  },
  {
    "title": "Swarm",
    "description": "**OpenAI** has released **Swarm** - an experimental, educational framework exploring ergonomic, lightweight multi-agent orchestration. \n\nKey abstractions:\n- **Agents**: Encompass instructions and tools\n- **Handoffs**: Allow agents to transfer conversation control\n\nThe framework is powered entirely by the Chat Completions API, remains stateless between calls, and is designed to showcase patterns from the *Orchestrating Agents* cookbook. Swarm builds on concepts from existing frameworks like AutoGen, CrewAI, and LangChain.",
    "launch_date": "2024-10-11",
    "launch_url": "https://github.com/openai/swarm",
    "tags": ["oss", "framework"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/openai/swarm",
    "company_url": "https://www.openai.com",
    "authors": ["OpenAI Solution team"],
    "image_url": null,
    "slug": "swarm-2024-10-11"
  },
  {
    "title": "ARIA: An Open Multimodal Native Mixture-of-Experts Model",
    "description": "**ARIA**, introduced by [Rhymes AI](https://rhymes.ai), is the first open mixture-of-experts (MoE) model that is truly multimodal native. With 3.9B activated parameters per visual token and 3.5B per text token, it outperforms larger open models like Pixtral-12B and Llama3.2-11B.\n\nKey features:\n- 64K token multimodal context window\n- Fine-grained MoE decoder with 66 experts per layer\n- Lightweight visual encoder with three resolution modes\n- Apache 2.0 licensed with comprehensive codebase\n\nThe model follows a innovative 4-stage training pipeline using 6.4T language tokens and 400B multimodal tokens, demonstrating strong performance in long-context multimodal understanding.",
    "launch_date": "2024-10-08",
    "launch_url": "https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model",
    "tags": ["research", "oss"],
    "paper_pdf_url": "https://arxiv.org/pdf/2410.05993.pdf",
    "tweet_url": null,
    "github_url": "https://github.com/rhymes-ai/Aria",
    "company_url": "https://rhymes.ai",
    "authors": [
      "Dongxu Li",
      "Yudong Liu",
      "Haoning Wu",
      "Yue Wang",
      "Zhiqi Shen",
      "Bowen Qu",
      "Xinyao Niu",
      "Guoyin Wang",
      "Bei Chen",
      "Junnan Li"
    ],
    "image_url": null,
    "slug": "aria-open-multimodal-native-mixture-of-experts-model-2024-10-08"
  },
  {
    "title": "Sema4.ai",
    "description": "**Enterprise AI Agents: The Next Generation of Business Applications**\n\nKey capabilities:\n- Perform complex work with unprecedented accuracy\n- Go beyond traditional chatbot capabilities\n- Focus on high-ROI work automation\n\nSema4.ai's agents boost productivity while maintaining precision, freeing human workers to focus on strategic tasks.",
    "launch_date": "2024-01-30",
    "launch_url": "https://sema4.ai/",
    "tags": ["startup", "product"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://sema4.ai/",
    "authors": null,
    "funding": [
      {
        "type": "SeriesA",
        "date": "2024-01-30",
        "launch_url": "https://www.mayfield.com/partnering-with-sema4-ai-to-accelerate-the-agent-economy/",
        "amount": 30050000
      }
    ],
    "image_url": null,
    "slug": "sema4-ai-agents-2024-01-30"
  },
  {
    "title": "Brightwave",
    "description": "**Brightwave** is developing an AI-powered financial research assistant that revolutionizes financial analysis through advanced AI systems. The platform synthesizes analysis from millions of sources including SEC filings, earnings calls, market data, and enterprise knowledge bases.\n\nKey applications:\n- Rapid sector and equity analysis\n- Private market opportunity discovery\n- Wealth management optimization\n- Automated market commentary\n\nFounded by Mike Conover (former Databricks LLM lead) and Brandon Kotara (former LedgerX CTO), with a team including NeurIPS-published researchers from Meta and Goldman Sachs.",
    "launch_date": "2024-06-11",
    "launch_url": "https://www.businesswire.com/news/home/20240611417446/en/",
    "tags": ["startup", "product", "fintech"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.brightwave.io/",
    "authors": null,
    "funding": [
      {
        "type": "Seed",
        "date": "2024-06-11",
        "launch_url": "https://www.businesswire.com/news/home/20240611417446/en/",
        "amount": 6000000
      }
    ],
    "image_url": null,
    "slug": "brightwave-financial-ai-assistant-2024-06-11"
  },
  {
    "title": "Magic",
    "description": "**Magic** is building a true AI colleague for software engineering, aiming to create aligned and complete AI to accelerate humanity's progress on the world's most challenging problems. The platform focuses on transforming software development through advanced AI collaboration.",
    "launch_date": "2023-02-06",
    "launch_url": "https://magic.dev/blog/series-a",
    "tags": ["startup", "product"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://magic.dev",
    "authors": ["Eric Steinberger", "Sebastian De Ro"],
    "funding": [
      {
        "type": "Seed",
        "date": "2022",
        "launch_url": null,
        "amount": 5000000
      },
      {
        "type": "SeriesA",
        "date": "2023-02-06",
        "launch_url": "https://magic.dev/blog/series-a",
        "amount": 23000000
      }
    ],
    "image_url": null,
    "slug": "magic-ai-software-engineer-2023-02-06"
  },
  {
    "title": "Movie Gen: A Cast of Media Foundation Models",
    "description": "**Movie Gen** is a groundbreaking suite of foundation models from Meta AI for advanced media generation. Built on a 30B parameter transformer model with 73K video token context length, it generates high-quality 1080p HD videos with synchronized audio.\n\nKey capabilities:\n- Text-to-video synthesis up to 16 seconds\n- Video personalization and editing\n- Audio generation and synchronization\n- Educational content development\n\nWhile not publicly released due to potential misuse concerns, Movie Gen sets new benchmarks in video quality and text alignment, demonstrating significant potential for multi-agent content creation systems.",
    "launch_date": "2024-10-04",
    "launch_url": "https://ai.meta.com/research/movie-gen/",
    "tags": ["research"],
    "paper_pdf_url": "https://ai.meta.com/static-resource/movie-gen-research-paper",
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://ai.meta.com/",
    "authors": ["The Movie Gen team @ Meta"],
    "image_url": null,
    "slug": "movie-gen-cast-of-media-foundation-models-2024-10-05"
  },
  {
    "title": "Molmo and PixMo",
    "description": "**Molmo** is a new family of state-of-the-art open multimodal AI models that achieve performance comparable to proprietary systems like GPT-4 and Gemini, while being fully open-source. What makes Molmo special is its novel training data called **PixMo** (Pixels for Molmo). Unlike conventional approaches that use billions of noisy web-scraped image-text pairs, PixMo focuses on quality over quantity, using less than 1 million highly curated image-text pairs.\n\nKey innovations:\n- Data collection through spoken 60-90 second image descriptions by human annotators\n- 'Modality switching trick' for more detailed, high-quality descriptions\n- Diverse supervised datasets for fine-tuning model capabilities\n- Groundbreaking pointing dataset for precise pixel-level interaction\n- Support for both physical and virtual environment interactions\n\nThis pointing capability enables Molmo to ground its understanding in precise pixel locations - for example, allowing robots to locate objects or web agents to identify UI elements. PixMo includes diverse supervised datasets for fine-tuning, creating a system that can perceive, reason about, and act in the world in more human-like ways. By combining high-quality training data with the ability to point and interact, Molmo represents a significant advance toward multimodal AI systems that can interface naturally with both physical and digital environments.",
    "launch_date": "2024-09-25",
    "launch_url": "https://molmo.allenai.org",
    "tags": ["research", "oss"],
    "paper_pdf_url": "https://molmo.allenai.org/paper.pdf",
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://allenai.org",
    "authors": [
      "Matt Deitke",
      "Christopher Clark",
      "Sangho Lee",
      "Rohun Tripathi",
      "Yue Yang",
      "James Park",
      "Reza Salehi",
      "Niklas Muennighoff",
      "Kyle Lo",
      "Luca Soldaini",
      "Jiasen Lu",
      "et al."
    ],
    "image_url": "default.png",
    "slug": "molmo-pixmo-2024-09-25"
  },
  {
    "title": "OmniParser for Pure Vision Based GUI Agent",
    "description": "OmniParser is a comprehensive method for parsing user interface screenshots into structured elements, significantly enhancing GPT-4V's ability to generate actions that can be accurately grounded in corresponding interface regions. The system introduces two key innovations: 1) reliable identification of interactable icons within user interfaces, and 2) improved understanding of element semantics in screenshots with accurate action-region association. The project includes a curated interactable icon detection dataset using popular webpages (67k unique screenshot images) and an icon description dataset (7k icon-description pairs). These datasets were used to fine-tune specialized models: a detection model for parsing interactable screen regions and a caption model for extracting functional semantics of detected elements. OmniParser significantly improves GPT-4V's performance on the ScreenSpot benchmark, and on Mind2Web and AITW benchmarks, it outperforms GPT-4V baselines using screenshot-only input. The system is designed to be plugin-ready for other vision language models, showing improved performance when combined with models like Phi-3.5-V and Llama-3.2-V.",
    "launch_date": "2024-08-01",
    "launch_url": "https://microsoft.github.io/OmniParser/",
    "tags": ["research", "oss"],
    "paper_pdf_url": "https://arxiv.org/pdf/2408.00203",
    "tweet_url": null,
    "github_url": "https://github.com/microsoft/OmniParser",
    "company_url": "https://www.microsoft.com/en-us/research/",
    "authors": ["Yadong Lu", "Jianwei Yang", "Yelong Shen", "Ahmed Awadallah"],
    "image_url": null,
    "slug": "omniparser-pure-vision-based-gui-agent-2024-08-01"
  },
  {
    "title": "Letta: Memory Management Platform for Agentic AI",
    "description": "Letta introduces a memory management platform for the next generation of agentic systems, building on research from the MemGPT project at UC Berkeley. It offers a production-ready platform for developers to create and launch stateful agents and LLM APIs. Key features include an Agent Development Environment (ADE) for developing, debugging, and deploying stateful agents, Letta Cloud for hosted services, and an open-source framework continuing the MemGPT project. Letta emphasizes a model-agnostic approach, allowing flexibility to switch between LLM providers, and focuses on innovations in the stateful layer above base LLMs.",
    "launch_date": "2024-09-23",
    "launch_url": "https://www.letta.com/blog/announcing-letta",
    "tags": ["product", "aiops", "startup"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.letta.com",
    "authors": [],
    "image_url": null,
    "slug": "letta-memory-management-platform-agentic-ai-2024-09-23"
  },
  {
    "title": "Agent Workflow Memory: Improving AI Agent Performance on Web Navigation Tasks",
    "description": "Researchers from Carnegie Mellon University and MIT have introduced Agent Workflow Memory (AWM), a method for inducing commonly reused routines (workflows) and selectively providing them to AI agents to guide subsequent actions. AWM can be applied in both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. Experiments on two major web navigation benchmarks — Mind2Web and WebArena — show that AWM substantially improves baseline results by 24.6% and 51.1% relative success rate respectively, while reducing the number of steps taken to solve tasks successfully. AWM also demonstrates robust generalization in cross-task, website, and domain evaluations, surpassing baselines by 8.9 to 14.0 absolute points as train-test task distribution gaps widen.",
    "launch_date": "2024-09-11",
    "launch_url": "https://arxiv.org/abs/2409.07429",
    "tags": ["research"],
    "paper_pdf_url": "https://arxiv.org/pdf/2409.07429.pdf",
    "tweet_url": null,
    "github_url": "https://github.com/zorazrw/agent-workflow-memory",
    "company_url": null,
    "authors": [
      "Zora Zhiruo Wang",
      "Jiayuan Mao",
      "Daniel Fried",
      "Graham Neubig"
    ],
    "image_url": null,
    "slug": "agent-workflow-memory-improving-ai-agent-performance-2024-09-11"
  },
  {
    "title": "CORE-Bench: A Benchmark for Computational Reproducibility Agents",
    "description": "Researchers from Princeton University have introduced **CORE-Bench** (Computational Reproducibility Agent Benchmark), a new benchmark for evaluating AI agents on computational reproducibility tasks. \n\nKey features:\n- 270 tasks from 90 scientific papers (CS, social science, medicine)\n- Three difficulty levels with language-only and vision-language tasks\n- Efficient evaluation system for measuring agent accuracy\n- Baseline testing with AutoGPT and CORE-Agent using GPT-4 variants\n\nInitial results showed only 21% accuracy on the hardest tasks, highlighting significant room for improvement. The benchmark aims to advance AI agents' capabilities in verifying and improving scientific research reproducibility.",
    "launch_date": "2024-09-17",
    "launch_url": "https://arxiv.org/abs/2409.11363",
    "tags": ["research", "benchmark"],
    "paper_pdf_url": "https://arxiv.org/pdf/2409.11363.pdf",
    "tweet_url": null,
    "github_url": "https://github.com/siegelz/core-bench",
    "company_url": "https://www.princeton.edu",
    "authors": [
      "Zachary S. Siegel",
      "Sayash Kapoor",
      "Nitya Nadgir",
      "Benedikt Stroebl",
      "Arvind Narayanan"
    ],
    "image_url": null,
    "slug": "core-bench-computational-reproducibility-agent-benchmark-2024-09-17"
  },
  {
    "title": "SWE-BENCH: Can Language Models Resolve Real-World GitHub Issues?",
    "description": "SWE-bench is an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Evaluations show that both state-of-the-art proprietary models and the fine-tuned SWE-Llama model can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues.",
    "launch_date": "2024-04-05",
    "launch_url": "https://www.swebench.com/",
    "tags": ["research", "benchmark", "oss"],
    "paper_pdf_url": "https://arxiv.org/pdf/2310.06770v2.pdf",
    "tweet_url": null,
    "github_url": "https://github.com/princeton-nlp/SWE-bench",
    "company_url": "https://www.swebench.com/",
    "authors": [
      "Carlos E. Jimenez",
      "John Yang",
      "Alexander Wettig",
      "Shunyu Yao",
      "Kexin Pei",
      "Ofir Press",
      "Karthik Narasimhan"
    ],
    "image_url": null,
    "slug": "swe-bench-2024-04-05"
  },
  {
    "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
    "description": "WebArena is a realistic and reproducible web environment designed to facilitate the development of autonomous agents capable of executing tasks. The environment comprises four fully operational, self-hosted web applications representing distinct domains prevalent on the internet: online shopping, discussion forums, collaborative development, and business content management. WebArena also incorporates utility tools and knowledge resources. Along with the environment, the authors release a benchmark with 812 long-horizon web-based tasks described as high-level natural language intents. The evaluation focuses on the functional correctness of task execution. Experiment results show that the best GPT-4 agent performance is limited, with an end-to-end task success rate of only 14.41%, while human performance is 78.24%.",
    "launch_date": "2024-04-16",
    "launch_url": "https://webarena.dev/",
    "tags": ["research", "benchmark"],
    "paper_pdf_url": "https://arxiv.org/pdf/2307.13854v4.pdf",
    "tweet_url": null,
    "github_url": "https://github.com/web-arena-x/webarena",
    "company_url": "https://www.cs.cmu.edu/",
    "authors": [
      "Shuyan Zhou",
      "Frank F. Xu",
      "Hao Zhu",
      "Xuhui Zhou",
      "Robert Lo",
      "Abishek Sridhar",
      "Xianyi Cheng",
      "Tianyue Ou",
      "Yonatan Bisk",
      "Daniel Fried",
      "Uri Alon",
      "Graham Neubig"
    ],
    "image_url": null,
    "slug": "webarena-realistic-web-environment-autonomous-agents-2024-04-16"
  },
  {
    "title": "Microsoft 365 Copilot Wave 2: Pages, Python in Excel, and agents",
    "description": "Microsoft is launching the next wave of Microsoft 365 Copilot, introducing significant updates to enhance AI-powered productivity. The key features include: 1) Copilot Pages, a dynamic, persistent canvas for multiplayer AI collaboration, designed as the first new digital artifact for the AI age. 2) Major improvements to Copilot in Microsoft 365 apps, such as advanced data analysis in Excel with Python support, dynamic storytelling in PowerPoint, and enhanced meeting capabilities in Teams. 3) Introduction of Copilot agents, which can automate and execute business processes, enabling teams to scale their efforts. The update also includes a new agent builder for creating custom Copilot agents. These enhancements are based on feedback from nearly 1,000 customers, resulting in over 700 product updates and 150 new features. Microsoft reports that Copilot responses are now more than two times faster on average, with response satisfaction improved by nearly three times.",
    "launch_date": "2024-09-16",
    "launch_url": "https://www.microsoft.com/en-us/microsoft-365/blog/2024/09/16/microsoft-365-copilot-wave-2-pages-python-in-excel-and-agents/",
    "tags": ["product", "enterprise"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.microsoft.com",
    "authors": ["Jared Spataro"],
    "image_url": "https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2024/09/M365_360536_Blog_240915-1-1024x576.webp",
    "slug": "microsoft-365-copilot-wave-2-2024-09-16"
  },
  {
    "title": "Agentforce: Creating AI Agents That Will Change Everything at Work",
    "description": "Salesforce is ushering in the third wave of the AI revolution, helping businesses deploy AI agents alongside human workers across the enterprise. These customizable agents can autonomously take action on tasks, freeing up workers to focus on the jobs only they can do. Built on the Salesforce Platform, Agentforce provides tools for creating and customizing AI agents that use Salesforce's Data Cloud to deliver contextual responses and proactively perform tasks within defined guardrails.",
    "launch_date": "2024-09-12",
    "launch_url": "https://www.salesforce.com/news/stories/building-business-ai-agents/",
    "tags": ["product"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.salesforce.com",
    "authors": [
      "Clara Shih",
      "Paula Goldman",
      "Mick Costigan",
      "Shelby Heinecke"
    ],
    "image_url": "default.png",
    "slug": "agentforce-2024-09-12"
  },
  {
    "title": "Windows Agent Arena: Benchmark for interface agents",
    "description": "Windows Agent Arena is an open-sourced benchmark that allows researchers to develop, test and compare AI agents for Windows. It evaluates models across a diverse range of tasks in a real OS using the same applications, tools, and web browsers available to users. The benchmark comes with 150+ agent tasks and allows parallelized evaluation in Azure.",
    "launch_date": "2024-09-13",
    "launch_url": "https://www.microsoft.com/applied-sciences/projects/windows-agent-arena",
    "tags": ["research", "oss"],
    "paper_pdf_url": "https://microsoft.github.io/WindowsAgentArena/static/files/windows_agent_arena.pdf",
    "tweet_url": null,
    "github_url": "https://github.com/microsoft/WindowsAgentArena",
    "company_url": "https://www.microsoft.com",
    "authors": [
      "Rogerio Bonatti",
      "Dan Zhao",
      "Francesco Bonacci",
      "Dillon Dupont",
      "Sara Abdali",
      "Yinheng Li",
      "Justin Wagle",
      "Kazuhito Koishida"
    ],
    "image_url": "default.png",
    "slug": "windows-agent-arena-2024-09-13"
  },
  {
    "title": "PaperQA2: AI agent for scientific literature research",
    "description": "PaperQA2 is also the first agent to beat PhD and Postdoc-level biology researchers on multiple literature research tasks, as measured both by accuracy on objective benchmarks and assessments by human experts. We are publishing a paper and open-sourcing the code.This is the first example of AI agents exceeding human performance on a major portion of scientific research, and will be a game-changer for the way humans interact with the scientific literature. ",
    "launch_date": "2024-09-11",
    "launch_url": "https://x.com/SGRodriques/status/1833908643856818443",
    "tags": ["research", "oss", "product"],
    "paper_pdf_url": null,
    "tweet_url": "https://x.com/SGRodriques/status/1833908643856818443",
    "github_url": "https://github.com/Future-House/paper-qa",
    "company_url": "https://www.futurehouse.org/",
    "authors": ["Sam  Rodriques", "et al."],
    "image_url": "default.png",
    "slug": "paperqa2-2024-09-11"
  },
  {
    "title": "SciAgents: Automating scientific discovery",
    "description": "SciAgents is an approach that leverages large-scale ontological knowledge graphs, large language models, and multi-agent systems with in-situ learning capabilities to automate scientific discovery. Applied to biologically inspired materials, it reveals hidden interdisciplinary relationships and autonomously generates and refines research hypotheses. The framework demonstrates the potential to accelerate materials discovery by combining generative AI, ontological representations, and multi-agent modeling.",
    "launch_date": "2024-09-09",
    "launch_url": "https://arxiv.org/abs/2409.05556",
    "tags": ["research"],
    "paper_pdf_url": "https://arxiv.org/pdf/2409.05556.pdf",
    "authors": [
      "Michael D. Skarlinski",
      "Sam Cox",
      "Jon M. Laurent",
      "James D. Braza",
      "Michaela Hinks",
      "Michael J. Hammerling",
      "Manvitha Ponnapati",
      "Samuel G. Rodriques",
      "Andrew D. White"
    ],
    "image_url": "default.png",
    "slug": "sciagents-2024-09-09"
  },
  {
    "title": "AutoGen",
    "description": "An Open-Source Programming Framework for Agentic AI",
    "launch_date": "2023-04-07",
    "launch_url": "https://github.com/microsoft/FLAML/commit/82f0a4309d2fe3f8445fcf48dca21eadc226e826",
    "tags": ["oss", "framework"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/microsoft/autogen",
    "company_url": null,
    "authors": [],
    "image_url": "default.png",
    "slug": "autogen-2023-04-07"
  },
  {
    "title": "AutoGPT",
    "description": "AutoGPT is a powerful tool that lets you create and run intelligent agents.",
    "launch_date": "2023-03-17",
    "launch_url": "https://github.com/Significant-Gravitas/AutoGPT/commits/master/?since=2023-03-16&until=2023-03-17",
    "tags": ["oss"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/Significant-Gravitas/AutoGPT",
    "company_url": null,
    "authors": [],
    "image_url": "default.png",
    "slug": "autogpt-2023-03-17"
  },
  {
    "title": "MultiOn: AI Agents for Task Automation",
    "description": "MultiOn is a pioneering AI startup developing AI agents that interact with the digital world to automate mundane tasks. Founded by Stanford alumni Div Garg and Omar Shaya, MultiOn aims to free up human time by handling tasks like booking travel itineraries or making restaurant reservations in minutes. The company's AI agents can perform actions across various digital platforms, from e-commerce to calendar management. MultiOn's mission is to unlock and elevate human agency, allowing people to focus on more meaningful activities. Backed by major investors including General Catalyst, Amazon Alexa Fund, and Samsung Next, MultiOn is not only developing consumer products but also an AI Agent platform for developers to create their own AI agents or integrate them into existing apps.",
    "launch_date": "2024-09-01",
    "launch_url": "https://www.multion.ai/blog/multion-building-a-brighter-future-for-humanity-with-ai-agents",
    "tags": ["product", "startup"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.multion.ai/",
    "authors": ["Div Garg", "Omar Shaya"],
    "image_url": "default.png",
    "slug": "multion-ai-agents-task-automation-2024-09-01"
  },
  {
    "title": "CrewAI",
    "description": "Framework for orchestrating role-playing, autonomous AI agents",
    "launch_date": "2023-10-29",
    "launch_url": "https://github.com/crewAIInc/crewAI/commits/main/?since=2023-10-27&until=2023-11-08",
    "tags": ["oss", "framework"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/crewAIInc/crewAI",
    "company_url": null,
    "authors": [],
    "image_url": "default.png",
    "slug": "crewai-2023-10-29"
  },
  {
    "title": "ChatDev",
    "description": "Communicative Agents for Software Development",
    "launch_date": "2023-11-27",
    "launch_url": "https://github.com/OpenBMB/ChatDev/commits/main/?since=2023-11-06&until=2023-11-10",
    "tags": ["oss", "framework"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/OpenBMB/ChatDev",
    "company_url": null,
    "authors": [],
    "image_url": "default.png",
    "slug": "chatdev-2023-11-27"
  },
  {
    "title": "Adept AI",
    "description": "Adept is building AI models that can use every software tool and API in the world, enabling a universal AI assistant that can complete any digital task. Their technology aims to automate complex software processes and workflows by understanding natural language commands and translating them into actions across various applications.",
    "launch_date": "2022-04-26",
    "launch_url": "https://techcrunch.com/2022/04/26/2304039/",
    "tags": ["startup", "product"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.adept.ai/",
    "authors": null,
    "funding": [
      {
        "type": "SeriesA",
        "date": "2022-04-26",
        "amount": 65000000,
        "launch_url": "https://techcrunch.com/2022/04/26/2304039/"
      },
      {
        "type": "SeriesB",
        "date": "2023-03-14",
        "amount": 350000000,
        "launch_url": "https://www.adept.ai/blog/series-b"
      }
    ],
    "image_url": null,
    "slug": "adept-ai-2022-04-26"
  },
  {
    "title": "Lindy AI",
    "description": "Build custom AI Assistants to automate any business workflow — no code required.",
    "launch_date": "2024-02-14",
    "launch_url": "https://www.lindy.ai/blog/what-is-an-emergency-department-scribe",
    "tags": ["product", "startup"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://lindy.ai/",
    "authors": [],
    "image_url": "default.png",
    "slug": "lindy-ai-2024-02-14"
  },
  {
    "title": "LangGraph",
    "description": "A library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows",
    "launch_date": "2023-08-23",
    "launch_url": "https://github.com/langchain-ai/langgraph/commits/main/?since=2023-08-09&until=2023-08-23",
    "tags": ["oss", "framework"],
    "paper_pdf_url": null,
    "tweet_url": null,
    "github_url": "https://github.com/langchain-ai/langgraph",
    "company_url": null,
    "authors": [],
    "image_url": "default.png",
    "slug": "langgraph-2023-08-23"
  },
  {
    "title": "Practices for Governing Agentic AI Systems",
    "description": "This research paper proposes practices for governing agentic AI systems to ensure their safe and accountable operation. It defines agentic AI systems, outlines potential benefits, and suggests governance practices to mitigate risks. The paper identifies key parties in the AI agent lifecycle and proposes seven practices for safety and accountability, including evaluating task suitability, constraining action-space, setting default behaviors, ensuring activity legibility, implementing automatic monitoring, establishing attributability, and maintaining interruptibility. It also discusses indirect impacts such as adoption races, labor displacement, shifting offense-defense balances, and correlated failures. The authors emphasize the need for ongoing development of best practices as AI capabilities advance.",
    "launch_date": "2023-12-01",
    "launch_url": "https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf",
    "tags": ["research", "responsibleai"],
    "paper_pdf_url": "https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf",
    "tweet_url": null,
    "github_url": null,
    "company_url": "https://www.openai.com",
    "authors": [
      "Yonadav Shavit",
      "Sandhini Agarwal",
      "Miles Brundage",
      "Steven Adler",
      "Cullen O'Keefe",
      "Rosie Campbell",
      "Teddy Lee",
      "Pamela Mishkin",
      "Tyna Eloundou",
      "Alan Hickey",
      "Katarina Slama"
    ],
    "image_url": null,
    "slug": "practices-for-governing-agentic-ai-systems-2023-12-01"
  }
]
