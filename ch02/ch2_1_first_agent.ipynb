{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Agent From Scratch\n",
    "\n",
    "This notebook demonstrates how to build an AI agent from the ground up, implementing the core components described in Chapter 2:\n",
    "\n",
    "* Defining agents in code\n",
    "* Granting agents access to tools\n",
    "* Implementing short and long term memory in agents\n",
    "* Exploring strategies for task termination\n",
    "* Function calling (tool calling) in LLMs\n",
    "\n",
    "By the end of this notebook, you will progressively build from an agent that can address simple tasks like \"what is the capital of France?\" to one that can \"act\" - such as \"plot the stock price of Microsoft over the last 5 years\" or \"generate a weather report\".\n",
    "\n",
    "![Agent Components](https://github.com/victordibia/multiagent-systems-with-autogen/blob/main/docs/images/agent.png?raw=true)\n",
    "\n",
    "## Installation\n",
    "\n",
    "We'll need the OpenAI library for the generative AI model and other basic dependencies.\n",
    "\n",
    "```bash\n",
    "pip install openai python-dotenv pydantic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U openai python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Before we build our agent, we need to set up our generative AI model. We'll use OpenAI's GPT models for this example.\n",
    "\n",
    "**API Key Setup:**\n",
    "- Create a new file called `.env` in the folder for this notebook\n",
    "- Add the following line to the file, replacing `YOUR_OPENAI_API_KEY` with your actual OpenAI API key:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=YOUR_OPENAI_API_KEY\n",
    "```\n",
    "- Save the file\n",
    "\n",
    "You can get your API key from [OpenAI](https://platform.openai.com/signup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an Agent in Code\n",
    "\n",
    "We'll start by defining a basic `Agent` class with core components: name, instructions, model, tools, memory, and message history. The `run` method will serve as the foundation for executing agent logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent assistant received task: What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Callable\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    " \n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    \"\"\"A class representing the memory of an agent.\"\"\"\n",
    "    pass\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, instructions: str, model: str = \"gpt-4.1-nano\", tools: List[Callable] | None = None, memory: Memory | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the agent with its name, instructions, model, tools, and memory.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.model_client = OpenAI()\n",
    "        self.instructions = instructions\n",
    "        self.tools = tools or []\n",
    "        self.memory = memory \n",
    "        self.message_history = []\n",
    "\n",
    "    def run(self, task: str) -> str:\n",
    "        \"\"\"\n",
    "        Logic for a single turn of the agent's behavior\n",
    "        \"\"\"\n",
    "        # For now, this is just a placeholder\n",
    "        return f\"Agent {self.name} received task: {task}\"\n",
    "\n",
    "# Create a basic agent\n",
    "agent = Agent(\n",
    "    name=\"assistant\", \n",
    "    instructions=\"You are a helpful assistant.\", \n",
    "    model=\"gpt-4.1-nano\"\n",
    ")\n",
    "\n",
    "# Test the basic structure\n",
    "response = agent.run(task=\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent + Generative AI Model\n",
    "\n",
    "Now let's extend our agent to actually use a generative AI model for reasoning and generating responses. We'll implement the `_call_model` method that communicates with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Callable\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, instructions: str, model: str = \"gpt-4.1-nano\", tools: List[Callable] | None = None, memory: Memory | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the agent with its name, instructions, model, tools, and memory.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.model_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "        self.instructions = instructions\n",
    "        self.tools = tools or []\n",
    "        self.memory = memory \n",
    "        self.message_history = []\n",
    "\n",
    "    def _call_model(self, messages: list) -> str:\n",
    "        \"\"\"\n",
    "        Call the model with the conversation history and return the generated response.\n",
    "        \"\"\"\n",
    "        response = self.model_client.responses.create(\n",
    "            model=self.model,\n",
    "            input=messages,\n",
    "            temperature=0.7,\n",
    "            max_output_tokens=150\n",
    "        )\n",
    "        return response.output_text.strip()\n",
    "\n",
    "    def run(self, task: str) -> str:\n",
    "        \"\"\"\n",
    "        Run the agent with the given task and return the response.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.instructions},\n",
    "            {\"role\": \"user\", \"content\": task}\n",
    "        ] + self.message_history \n",
    "\n",
    "        response = self._call_model(messages)\n",
    "        self.message_history.append({\"role\": \"user\", \"content\": task})\n",
    "        self.message_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "\n",
    "# Create and test the improved agent\n",
    "agent = Agent(\n",
    "    name=\"assistant\", \n",
    "    instructions=\"You are a helpful assistant.\", \n",
    "    model=\"gpt-4.1-nano\"\n",
    ")\n",
    "\n",
    "# Test with a simple question\n",
    "response = agent.run(task=\"What is the capital of France?\")\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent + Tools\n",
    "\n",
    "Tools expand the range of tasks that an agent can perform beyond text generation. Let's explore how to give our agent access to tools.\n",
    "\n",
    "### Understanding Tool Categories\n",
    "\n",
    "**Task-specific tools** provide capabilities for specific tasks (e.g., weather API, stock prices)\n",
    "**General-purpose tools** offer broad capabilities (e.g., code interpreters, UI drivers)\n",
    "\n",
    "### Function Calling in LLMs\n",
    "\n",
    "Modern LLMs can translate natural language requests into structured function calls. This involves:\n",
    "1. **Function definition**: Define the function with name, description, and parameters\n",
    "2. **Register with LLM**: Present function definitions to the model\n",
    "3. **Generate function call**: LLM creates structured function calls based on user requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tools defined: get_weather, get_stock_price\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import inspect\n",
    "\n",
    "# Define some example tools that we'll use with our agent\n",
    "def get_weather(location: str, date_range: str) -> str:\n",
    "    \"\"\"Get the weather for a given location and date range.\"\"\"\n",
    "    return f\"The weather in {location} from {date_range} is sunny with a high of 75Â°F.\"\n",
    "\n",
    "def get_stock_price(ticker: str, date_range: str) -> str:\n",
    "    \"\"\"Get the stock price for a given ticker and date range.\"\"\"\n",
    "    return f\"The stock price for {ticker} from {date_range} averaged $150.\"\n",
    "\n",
    "print(\"Example tools defined: get_weather, get_stock_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name: str, instructions: str, model: str = \"gpt-4.1-nano\", tools: List[Callable] = None, memory: Memory = None):\n",
    "        \"\"\"\n",
    "        Initialize the agent with its name, instructions, model, tools, and memory.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.model_client = OpenAI()\n",
    "        self.instructions = instructions\n",
    "        self.tools = tools or []\n",
    "        self.memory = memory \n",
    "        self.message_history = []\n",
    "        \n",
    "        # Create a mapping of tool names to functions\n",
    "        self.tool_map = {tool.__name__: tool for tool in self.tools}\n",
    "\n",
    "    def _convert_tools_to_llm_format(self, tools: List[Callable]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Convert a list of tool functions to a format suitable for the LLM.\n",
    "        \"\"\"\n",
    "        llm_tools = []\n",
    "        for tool in tools:\n",
    "            # Get function signature\n",
    "            sig = inspect.signature(tool)\n",
    "            parameters = {\n",
    "                \"type\": \"object\", \n",
    "                \"properties\": {}, \n",
    "                \"required\": [],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "            \n",
    "            for param_name, param in sig.parameters.items():\n",
    "                parameters[\"properties\"][param_name] = {\n",
    "                    \"type\": \"string\", \n",
    "                    \"description\": f\"The {param_name} parameter\"\n",
    "                }\n",
    "                parameters[\"required\"].append(param_name)\n",
    "            \n",
    "            tool_info = {\n",
    "                \"type\": \"function\",\n",
    "                \"name\": tool.__name__,\n",
    "                \"description\": tool.__doc__ or f\"Function {tool.__name__}\",\n",
    "                \"parameters\": parameters,\n",
    "                \"strict\": True\n",
    "            }\n",
    "            llm_tools.append(tool_info)\n",
    "        return llm_tools\n",
    "\n",
    "    def _execute_tool(self, tool_call: dict) -> str:\n",
    "        \"\"\"\n",
    "        Execute the tool call and return the result.\n",
    "        \"\"\"\n",
    "        function_name = tool_call.get(\"name\") or tool_call.get(\"function\", {}).get(\"name\")\n",
    "        function_args = tool_call.get(\"arguments\") or tool_call.get(\"function\", {}).get(\"arguments\")\n",
    "        \n",
    "        # Handle both string and dict argument formats\n",
    "        if isinstance(function_args, str):\n",
    "            function_args = json.loads(function_args)\n",
    "        \n",
    "        if function_name in self.tool_map:\n",
    "            function_to_call = self.tool_map[function_name]\n",
    "            try:\n",
    "                result = function_to_call(**function_args)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                return f\"Error executing {function_name}: {str(e)}\"\n",
    "        else:\n",
    "            return f\"Unknown function: {function_name}\"\n",
    "\n",
    "    def _call_model(self, messages: list) -> str:\n",
    "        \"\"\"\n",
    "        Call the model with the conversation history and return the generated response.\n",
    "        \"\"\"\n",
    "        # Convert tools to LLM format if tools are available\n",
    "        llm_tools = self._convert_tools_to_llm_format(self.tools) if self.tools else None\n",
    "        \n",
    "        # Prepare request parameters\n",
    "        request_params = {\n",
    "            \"model\": self.model,\n",
    "            \"input\": messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_output_tokens\": 150\n",
    "        }\n",
    "        \n",
    "        # Only add tools if they exist\n",
    "        if llm_tools:\n",
    "            request_params[\"tools\"] = llm_tools\n",
    "        \n",
    "        response = self.model_client.responses.create(**request_params)\n",
    "        \n",
    "        # Process response \n",
    "        if response.output:\n",
    "            first_output = response.output[0]\n",
    "            \n",
    "            # Check if this is a message with tool calls\n",
    "            if hasattr(first_output, 'type') and first_output.type == \"message\":\n",
    "                # Check for tool calls in the message content\n",
    "                for content_item in first_output.content:\n",
    "                    if hasattr(content_item, 'type') and content_item.type == \"tool_call\":\n",
    "                        # Execute the tool call\n",
    "                        tool_result = self._execute_tool(content_item)\n",
    "                        \n",
    "                        # Add tool call and result to conversation\n",
    "                        messages.append({\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": [{\"type\": \"tool_call\", \"name\": content_item.name, \"arguments\": content_item.arguments}]\n",
    "                        })\n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": tool_result\n",
    "                        })\n",
    "                        \n",
    "                        # Get final response from model\n",
    "                        final_response = self.model_client.responses.create(**request_params)\n",
    "                        return final_response.output_text\n",
    "                \n",
    "                # If no tool calls, extract text content\n",
    "                for content_item in first_output.content:\n",
    "                    if hasattr(content_item, 'type') and content_item.type == \"output_text\":\n",
    "                        return content_item.text\n",
    "        \n",
    "        # Fallback to output_text property\n",
    "        return response.output_text\n",
    "\n",
    "    def run(self, task: str) -> str:\n",
    "        \"\"\"\n",
    "        Run the agent with the given task and return the response.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.instructions},\n",
    "        ] + self.message_history + [\n",
    "            {\"role\": \"user\", \"content\": task}\n",
    "        ]\n",
    "\n",
    "        response = self._call_model(messages)\n",
    "        self.message_history.append({\"role\": \"user\", \"content\": task})\n",
    "        self.message_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with 2 tools: ['get_weather', 'get_stock_price']\n",
      "Tool mapping: ['get_weather', 'get_stock_price']\n",
      "\n",
      "=== Weather Request ===\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for function 'get_weather': In context=(), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[0].parameters', 'code': 'invalid_function_parameters'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test with weather request\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Weather Request ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m response1 \u001b[38;5;241m=\u001b[39m \u001b[43magent_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the weather going to be like in San Francisco over the next 4 days?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Stock Request ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 130\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mRun the agent with the given task and return the response.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    125\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstructions},\n\u001b[1;32m    126\u001b[0m ] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_history \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    127\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: task}\n\u001b[1;32m    128\u001b[0m ]\n\u001b[0;32m--> 130\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: task})\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: response})\n",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m, in \u001b[0;36mAgent._call_model\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_tools:\n\u001b[1;32m     82\u001b[0m     request_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m llm_tools\n\u001b[0;32m---> 84\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Process response \u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39moutput:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/agnext/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/agnext/lib/python3.11/site-packages/openai/resources/responses/responses.py:656\u001b[0m, in \u001b[0;36mResponses.create\u001b[0;34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    655\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response \u001b[38;5;241m|\u001b[39m Stream[ResponseStreamEvent]:\n\u001b[0;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/responses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_output_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_response_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/agnext/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/agnext/lib/python3.11/site-packages/openai/_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for function 'get_weather': In context=(), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[0].parameters', 'code': 'invalid_function_parameters'}}"
     ]
    }
   ],
   "source": [
    "# Create agent with tools - now the agent handles tool conversion internally\n",
    "agent_with_tools = Agent(\n",
    "    name=\"assistant\", \n",
    "    instructions=\"You are a helpful assistant that can get weather and stock information.\", \n",
    "    model=\"gpt-4.1-nano\",\n",
    "    tools=[get_weather, get_stock_price]  # Simply pass the list of tools\n",
    ")\n",
    "\n",
    "print(f\"Agent created with {len(agent_with_tools.tools)} tools: {[tool.__name__ for tool in agent_with_tools.tools]}\")\n",
    "print(f\"Tool mapping: {list(agent_with_tools.tool_map.keys())}\")\n",
    "\n",
    "# Test with weather request\n",
    "print(\"\\n=== Weather Request ===\")\n",
    "response1 = agent_with_tools.run(task=\"What is the weather going to be like in San Francisco over the next 4 days?\")\n",
    "print(f\"Agent response: {response1}\")\n",
    "\n",
    "print(\"\\n=== Stock Request ===\")\n",
    "response2 = agent_with_tools.run(task=\"What was the stock price of AAPL last week?\")\n",
    "print(f\"Agent response: {response2}\")\n",
    "\n",
    "print(\"\\n=== General Question ===\")\n",
    "response3 = agent_with_tools.run(task=\"What is the capital of France?\")\n",
    "print(f\"Agent response: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Feedback Tools\n",
    "\n",
    "Agents can request input or clarification from humans when tasks are complex, ambiguous, or require human oversight. This introduces a human-in-the-loop component for enhanced problem-solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_tool(message: str) -> str:\n",
    "    \"\"\"\n",
    "    A tool that requests human feedback when the agent needs clarification or approval.\n",
    "    \"\"\"\n",
    "    print(f\"\\nð¤ Agent needs human input: {message}\")\n",
    "    response = input(\"ð¤ Your response: \")\n",
    "    return response\n",
    "\n",
    "def send_email(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"\n",
    "    Send an email (simulated - requires human approval for sensitive actions).\n",
    "    \"\"\"\n",
    "    return f\"Email draft created. To: {recipient}, Subject: {subject}, Body: {body[:50]}...\"\n",
    "\n",
    "# Create agent with human feedback capability\n",
    "agent_with_feedback = Agent(\n",
    "    name=\"assistant\", \n",
    "    instructions=\"You are a helpful assistant. For sensitive actions like sending emails, always ask for human approval using the human_feedback_tool.\", \n",
    "    model=\"gpt-4.1-nano\",\n",
    "    tools=[get_weather, send_email, human_feedback_tool]\n",
    ")\n",
    "\n",
    "# Example of human-in-the-loop interaction\n",
    "print(\"=== Example with Human Feedback ===\")\n",
    "# Note: This would normally prompt for human input in an interactive environment\n",
    "# For demonstration, we'll show the structure\n",
    "response = agent_with_feedback.run(task=\"Can you help me send an email to john@example.com about tomorrow's meeting?\")\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory and Conversation Context\n",
    "\n",
    "Our agents maintain conversation history in the `message_history` attribute. This enables:\n",
    "\n",
    "1. **Short-term memory**: Maintaining context within a conversation\n",
    "2. **Reference to previous interactions**: Agents can refer back to earlier parts of the conversation\n",
    "3. **Continuous learning**: Agents can build upon previous exchanges\n",
    "\n",
    "Let's demonstrate how memory works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new agent for memory demonstration\n",
    "memory_agent = Agent(\n",
    "    name=\"memory_agent\", \n",
    "    instructions=\"You are a helpful assistant with good memory. Remember details from our conversation.\", \n",
    "    model=\"gpt-4.1-nano\"\n",
    ")\n",
    "\n",
    "print(\"=== Memory Demonstration ===\")\n",
    "\n",
    "# First interaction\n",
    "response1 = memory_agent.run(\"My name is Alice and I'm working on a Python project about weather data.\")\n",
    "print(f\"Agent: {response1}\")\n",
    "\n",
    "# Second interaction - agent should remember Alice's name and project\n",
    "response2 = memory_agent.run(\"What programming language am I using for my project?\")\n",
    "print(f\"Agent: {response2}\")\n",
    "\n",
    "# Third interaction - testing deeper memory\n",
    "response3 = memory_agent.run(\"What's my name again?\")\n",
    "print(f\"Agent: {response3}\")\n",
    "\n",
    "# Show the message history\n",
    "print(f\"\\n=== Message History (Length: {len(memory_agent.message_history)}) ===\")\n",
    "for i, msg in enumerate(memory_agent.message_history):\n",
    "    print(f\"{i+1}. {msg['role']}: {msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've built an AI agent from scratch, implementing the core concepts from Chapter 2:\n",
    "\n",
    "### Key Components Implemented:\n",
    "\n",
    "1. **Basic Agent Class**: Defined with name, instructions, model, tools, and memory\n",
    "2. **Generative AI Integration**: Added `_call_model` method for LLM communication  \n",
    "3. **Tool System**: \n",
    "   - Internal tool conversion to LLM format\n",
    "   - Function calling capabilities\n",
    "   - Task-specific tools (weather, stock prices)\n",
    "   - Human feedback tools for human-in-the-loop interactions\n",
    "4. **Memory**: Conversation history for context maintenance\n",
    "5. **Progressive Complexity**: From simple text responses to tool-enabled actions\n",
    "\n",
    "### From Simple to Complex:\n",
    "\n",
    "- â **Simple**: \"What is the capital of France?\" â Text generation\n",
    "- â **Tools**: \"What's the weather in San Francisco?\" â Function calling  \n",
    "- â **Memory**: Multi-turn conversations with context\n",
    "- â **Human Feedback**: Requesting approval for sensitive actions\n",
    "\n",
    "This foundation demonstrates how agents can be built from first principles, providing full control over behavior, tool access, and interaction patterns. The concepts here form the basis for more sophisticated multi-agent systems covered in later chapters.\n",
    "\n",
    "### Next Steps:\n",
    "- Explore more complex tool integrations\n",
    "- Implement different memory strategies  \n",
    "- Add error handling and robustness\n",
    "- Scale to multiple collaborating agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agnext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
