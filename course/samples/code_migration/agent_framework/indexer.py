# Copyright (c) Microsoft. All rights reserved.
"""
Indexer for Agent Framework samples.

Scans Python and .NET sample directories, extracts metadata, and uses
Azure OpenAI with structured outputs to generate enhanced descriptions and tags.

Features:
- Idempotent: Only processes new or unprocessed samples
- Threaded: Parallel LLM calls for faster processing
- Saves progress incrementally to index.json
"""

import argparse
import asyncio
import json
import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from threading import Lock
from typing import Annotated

from agent_framework.azure import AzureOpenAIChatClient 
from pydantic import BaseModel, Field

# Configuration - will be set from command line args
REPO_ROOT: Path = None  # type: ignore
PYTHON_SAMPLES_DIR: Path = None  # type: ignore
DOTNET_SAMPLES_DIR: Path = None  # type: ignore
INDEX_FILE = Path(__file__).parent / "index.json"
GITHUB_BASE_URL = "https://github.com/microsoft/agent-framework/blob/main"
GITHUB_RAW_URL = "https://raw.githubusercontent.com/microsoft/agent-framework/main"
MAX_WORKERS = 5  # Concurrent LLM calls

# Thread-safe lock for writing to index
index_lock = Lock()


class SampleMetadata(BaseModel):
    """Enhanced metadata generated by LLM."""

    description: str = Field(
        description="Rich 2-3 sentence description explaining what this sample demonstrates, "
        "key concepts covered, and when developers should use it. Focus on migration use cases."
    )
    tags: list[str] = Field(
        description="Exactly 4 relevant tags for filtering. Examples: 'agent', 'workflow', "
        "'middleware', 'beginner', 'advanced', 'semantic_kernel_migration', 'autogen_migration', "
        "'chat', 'tools', 'parallel', 'multimodal', etc."
    )


class SampleEntry(BaseModel):
    """Complete sample entry in the index."""

    name: str
    category: str
    file_path: str
    github_url: str
    language: str
    description: str = ""
    tags: list[str] = []
    processed: bool = False


def scan_python_samples(repo_root: Path) -> list[dict]:
    """Scan Python samples directory and extract basic metadata."""
    samples = []
    python_samples_dir = repo_root / "python/samples/getting_started"

    if not python_samples_dir.exists():
        print(f"Warning: Python samples directory not found: {python_samples_dir}")
        return samples

    for py_file in python_samples_dir.rglob("*.py"):
        # Skip __init__.py and files in __pycache__
        if py_file.name == "__init__.py" or "__pycache__" in str(py_file):
            continue

        # Extract category from directory structure
        rel_path = py_file.relative_to(python_samples_dir)
        category = rel_path.parts[0] if len(rel_path.parts) > 1 else "uncategorized"

        # Generate name from filename
        name = py_file.stem

        # Create relative path from repo root
        file_path = str(py_file.relative_to(repo_root))
        github_url = f"{GITHUB_BASE_URL}/{file_path}"

        samples.append(
            {
                "name": name,
                "category": category,
                "file_path": file_path,
                "github_url": github_url,
                "language": "python",
                "processed": False,
            }
        )

    return samples


def scan_dotnet_samples(repo_root: Path) -> list[dict]:
    """Scan .NET samples directory and extract basic metadata."""
    samples = []
    dotnet_samples_dir = repo_root / "dotnet/samples"

    if not dotnet_samples_dir.exists():
        print(f"Warning: .NET samples directory not found: {dotnet_samples_dir}")
        return samples

    for cs_file in dotnet_samples_dir.rglob("*.cs"):
        # Skip auto-generated files
        if "obj/" in str(cs_file) or "bin/" in str(cs_file):
            continue

        # Extract category from directory structure
        rel_path = cs_file.relative_to(dotnet_samples_dir)
        category = rel_path.parts[0] if len(rel_path.parts) > 1 else "uncategorized"

        # Generate name from filename
        name = cs_file.stem

        # Create relative path from repo root
        file_path = str(cs_file.relative_to(repo_root))
        github_url = f"{GITHUB_BASE_URL}/{file_path}"

        samples.append(
            {
                "name": name,
                "category": category,
                "file_path": file_path,
                "github_url": github_url,
                "language": "dotnet",
                "processed": False,
            }
        )

    return samples


def scan_documentation(repo_root: Path, language: str) -> list[dict]:
    """Scan for README.md files in sample directories."""
    docs = []

    if language == "python":
        samples_dir = repo_root / "python/samples/getting_started"
    elif language == "dotnet":
        samples_dir = repo_root / "dotnet/samples"
    else:
        return docs

    if not samples_dir.exists():
        print(f"Warning: {language} samples directory not found: {samples_dir}")
        return docs

    # Find all README.md files
    for readme_file in samples_dir.rglob("README.md"):
        # Extract category from directory structure
        rel_path = readme_file.relative_to(samples_dir)

        # Get category from parent directory
        if len(rel_path.parts) > 1:
            category = rel_path.parts[0]
        else:
            category = "root"  # Root README

        # Generate name from directory path
        if category == "root":
            name = f"{language}_overview"
        else:
            name = f"{category}_documentation"

        # Create relative path from repo root
        file_path = str(readme_file.relative_to(repo_root))
        github_url = f"{GITHUB_BASE_URL}/{file_path}"

        docs.append(
            {
                "name": name,
                "category": category,
                "file_path": file_path,
                "github_url": github_url,
                "language": language,
                "type": "documentation",
                "processed": False,
            }
        )

    return docs


def extract_file_content(file_path: str, repo_root: Path, max_lines: int = 150) -> str:
    """Extract file content for LLM analysis (first N lines)."""
    try:
        full_path = repo_root / file_path
        with open(full_path, "r", encoding="utf-8") as f:
            lines = f.readlines()[:max_lines]
            return "".join(lines)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""


def extract_readme_content(file_path: str, repo_root: Path, max_chars: int = 8000) -> str:
    """Extract README content for LLM analysis (limited by characters, not lines)."""
    try:
        full_path = repo_root / file_path
        with open(full_path, "r", encoding="utf-8") as f:
            content = f.read()
            # Limit to max_chars to avoid token limits
            if len(content) > max_chars:
                content = content[:max_chars] + "\n... (truncated)"
            return content
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""


async def enhance_documentation_with_llm(doc: dict, repo_root: Path) -> dict:
    """Use Azure OpenAI to generate metadata from README documentation."""
    readme_content = extract_readme_content(doc["file_path"], repo_root)

    if not readme_content:
        print(f"Skipping {doc['name']}: Could not read README content")
        doc["processed"] = True
        return doc

    # Create prompt for LLM
    prompt = f"""Analyze this README documentation from the Agent Framework and provide metadata for a migration guide index.

Documentation Category: {doc['category']}
Documentation Name: {doc['name']}
File Path: {doc['file_path']}

README Content:
```markdown
{readme_content}
```

Generate:
1. A rich 2-3 sentence description that explains:
   - What concepts or features this documentation covers
   - Key Agent Framework capabilities explained
   - When developers migrating from other frameworks (AutoGen, Semantic Kernel, LangChain) should reference this documentation

2. Exactly 5-7 relevant tags for filtering. Be specific and use tags like:
   - Documentation type: 'documentation', 'guide', 'overview', 'tutorial'
   - Framework level: 'agent', 'workflow', 'middleware', 'tools', 'chat_client'
   - Skill level: 'beginner', 'intermediate', 'advanced'
   - Migration sources: 'semantic_kernel_migration', 'autogen_migration', 'langchain_migration'
   - Capabilities: 'multimodal', 'parallel', 'streaming', 'structured_output', 'observability'
   - Patterns: 'human_in_loop', 'fan_out', 'checkpoint', 'reflection'
"""

    try:
        # Use Azure OpenAI Client with structured output
        client = AzureOpenAIChatClient()
        agent = client.create_agent(
            name="DocumentationAnalyzer",
            instructions="You are an expert at analyzing technical documentation and creating helpful metadata for migration guides.",
        )

        result = await agent.run(prompt, response_format=SampleMetadata)

        if result.value:
            metadata: SampleMetadata = result.value  # type: ignore
            doc["description"] = metadata.description
            doc["tags"] = metadata.tags
            doc["processed"] = True
            print(f"✓ Processed doc: {doc['name']}")
        else:
            print(f"✗ Failed doc: {doc['name']} (no value in result)")
            doc["processed"] = False  # Allow retry

    except Exception as e:
        print(f"✗ Error processing doc {doc['name']}: {e}")
        doc["processed"] = False  # Allow retry

    return doc


async def enhance_with_llm(sample: dict, repo_root: Path) -> dict:
    """Use Azure OpenAI to generate enhanced description and tags for code samples."""
    file_content = extract_file_content(sample["file_path"], repo_root)

    if not file_content:
        print(f"Skipping {sample['name']}: Could not read file content")
        sample["processed"] = True  # Mark as processed even if failed
        return sample

    # Create prompt for LLM
    prompt = f"""Analyze this {sample['language']} code sample and provide metadata for a migration guide index.

Sample Category: {sample['category']}
Sample Name: {sample['name']}
File Path: {sample['file_path']}

Code Preview (first 150 lines):
```
{file_content}
```

Generate:
1. A rich 2-3 sentence description that explains:
   - What this sample demonstrates
   - Key Agent Framework concepts it covers
   - When developers migrating from other frameworks (AutoGen, Semantic Kernel, LangChain) should reference this

2. Exactly 4 relevant tags for filtering. Be specific and use tags like:
   - Framework level: 'agent', 'workflow', 'middleware', 'tools', 'chat_client'
   - Skill level: 'beginner', 'intermediate', 'advanced'
   - Migration sources: 'semantic_kernel_migration', 'autogen_migration', 'langchain_migration'
   - Capabilities: 'multimodal', 'parallel', 'streaming', 'structured_output', 'observability'
   - Patterns: 'human_in_loop', 'fan_out', 'checkpoint', 'reflection'
"""

    try:
        # Use OpenAI Responses Client with structured output
        client = AzureOpenAIChatClient()
        agent = client.create_agent(
            name="SampleAnalyzer",
            instructions="You are an expert at analyzing code samples and creating helpful documentation metadata.",
        )

        result = await agent.run(prompt, response_format=SampleMetadata)

        if result.value:
            metadata: SampleMetadata = result.value  # type: ignore
            sample["description"] = metadata.description
            sample["tags"] = metadata.tags
            sample["processed"] = True
            print(f"✓ Processed: {sample['name']}")
        else:
            print(f"✗ Failed: {sample['name']} (no value in result)")
            # Don't mark as processed - LLM returned no value, could retry
            sample["processed"] = False

    except Exception as e:
        error_msg = str(e).lower()

        # Check if this is a retriable error (client configuration, network, rate limits)
        retriable_errors = [
            "model id is required",
            "api key",
            "authentication",
            "credential",
            "rate limit",
            "timeout",
            "connection",
            "network",
            "service unavailable",
            "internal server error"
        ]

        is_retriable = any(err in error_msg for err in retriable_errors)

        if is_retriable:
            print(f"✗ Error processing {sample['name']}: {e}")
            print(f"  → Retriable error - will retry on next run")
            sample["processed"] = False  # Keep as unprocessed for retry
        else:
            print(f"✗ Error processing {sample['name']}: {e}")
            print(f"  → Permanent error - marking as processed")
            sample["processed"] = True  # Permanent failure, don't retry

    return sample


def load_existing_index() -> dict:
    """Load existing index.json if it exists."""
    if INDEX_FILE.exists():
        try:
            with open(INDEX_FILE, "r") as f:
                return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load existing index: {e}")
            return {"version": "1.0", "python": [], "dotnet": []}
    return {"version": "1.0", "python": [], "dotnet": []}


def save_index(index_data: dict) -> None:
    """Save index to JSON file (thread-safe)."""
    with index_lock:
        index_data["generated_at"] = datetime.utcnow().isoformat() + "Z"
        with open(INDEX_FILE, "w") as f:
            json.dump(index_data, f, indent=2)
        print(f"💾 Saved index to {INDEX_FILE}")


def merge_samples(existing: list[dict], new: list[dict]) -> list[dict]:
    """Merge new samples with existing, preserving processed entries."""
    existing_map = {s["file_path"]: s for s in existing}

    for sample in new:
        if sample["file_path"] not in existing_map:
            existing_map[sample["file_path"]] = sample

    return list(existing_map.values())


async def process_sample_async(sample: dict, repo_root: Path) -> dict:
    """Async wrapper for processing a single sample or documentation."""
    if sample.get("processed"):
        return sample

    # Route to appropriate enhancement function based on type
    if sample.get("type") == "documentation":
        return await enhance_documentation_with_llm(sample, repo_root)
    else:
        return await enhance_with_llm(sample, repo_root)


async def main():
    """Main indexer workflow."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Index Agent Framework samples for the Migration Assistant agent",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python indexer.py /path/to/agent-framework
  python indexer.py ~/repos/agent-framework
  python indexer.py .  # if running from agent-framework root

The indexer will:
1. Scan Python and .NET samples in the specified repo
2. Generate descriptions and tags using Azure OpenAI
3. Save index.json in the same directory as this script
        """
    )
    parser.add_argument(
        "repo_path",
        type=str,
        help="Path to the agent-framework repository root directory"
    )

    args = parser.parse_args()
    repo_root = Path(args.repo_path).resolve()

    # Validate repo path
    if not repo_root.exists():
        print(f"❌ Error: Repository path does not exist: {repo_root}")
        sys.exit(1)

    if not repo_root.is_dir():
        print(f"❌ Error: Repository path is not a directory: {repo_root}")
        sys.exit(1)

    # Check if it looks like the agent-framework repo
    python_samples_dir = repo_root / "python/samples/getting_started"
    dotnet_samples_dir = repo_root / "dotnet/samples"

    if not python_samples_dir.exists() and not dotnet_samples_dir.exists():
        print(f"⚠️  Warning: Neither Python nor .NET samples directory found at:")
        print(f"   {python_samples_dir}")
        print(f"   {dotnet_samples_dir}")
        print(f"\nAre you sure this is the agent-framework repository root?")
        response = input("Continue anyway? (y/N): ")
        if response.lower() != 'y':
            sys.exit(1)

    print("🚀 Agent Framework Sample Indexer")
    print("=" * 60)
    print(f"Repository: {repo_root}")
    print(f"Index output: {INDEX_FILE}")

    # Load existing index
    print("\n📂 Loading existing index...")
    index_data = load_existing_index()

    # Scan code samples
    print("\n🔍 Scanning Python samples...")
    python_samples = scan_python_samples(repo_root)
    print(f"   Found {len(python_samples)} Python code samples")

    print("\n🔍 Scanning .NET samples...")
    dotnet_samples = scan_dotnet_samples(repo_root)
    print(f"   Found {len(dotnet_samples)} .NET code samples")

    # Scan documentation
    print("\n📚 Scanning Python documentation (README files)...")
    python_docs = scan_documentation(repo_root, "python")
    print(f"   Found {len(python_docs)} Python README files")

    print("\n📚 Scanning .NET documentation (README files)...")
    dotnet_docs = scan_documentation(repo_root, "dotnet")
    print(f"   Found {len(dotnet_docs)} .NET README files")

    # Merge with existing (combine code samples and documentation)
    index_data["python"] = merge_samples(index_data.get("python", []), python_samples + python_docs)
    index_data["dotnet"] = merge_samples(index_data.get("dotnet", []), dotnet_samples + dotnet_docs)

    # Filter unprocessed
    unprocessed_python = [s for s in index_data["python"] if not s.get("processed")]
    unprocessed_dotnet = [s for s in index_data["dotnet"] if not s.get("processed")]
    total_unprocessed = len(unprocessed_python) + len(unprocessed_dotnet)

    # Count docs vs code samples
    python_docs_unprocessed = [s for s in unprocessed_python if s.get("type") == "documentation"]
    python_code_unprocessed = [s for s in unprocessed_python if s.get("type") != "documentation"]
    dotnet_docs_unprocessed = [s for s in unprocessed_dotnet if s.get("type") == "documentation"]
    dotnet_code_unprocessed = [s for s in unprocessed_dotnet if s.get("type") != "documentation"]

    print(f"\n📊 Status:")
    print(f"   Python: {len(unprocessed_python)} unprocessed / {len(index_data['python'])} total")
    print(f"      - Code samples: {len(python_code_unprocessed)} unprocessed")
    print(f"      - Documentation: {len(python_docs_unprocessed)} unprocessed")
    print(f"   .NET: {len(unprocessed_dotnet)} unprocessed / {len(index_data['dotnet'])} total")
    print(f"      - Code samples: {len(dotnet_code_unprocessed)} unprocessed")
    print(f"      - Documentation: {len(dotnet_docs_unprocessed)} unprocessed")

    if total_unprocessed == 0:
        print("\n✅ All samples and documentation already processed!")
        return

    # Process unprocessed samples
    print(f"\n🤖 Enhancing {total_unprocessed} samples with Azure OpenAI...")
    print(f"   Using {MAX_WORKERS} concurrent workers\n")

    all_unprocessed = unprocessed_python + unprocessed_dotnet

    # Process samples in batches with asyncio
    processed = []
    for i in range(0, len(all_unprocessed), MAX_WORKERS):
        batch = all_unprocessed[i : i + MAX_WORKERS]
        results = await asyncio.gather(*[process_sample_async(s, repo_root) for s in batch])
        processed.extend(results)

        # Save progress after each batch
        for sample in results:
            for idx, existing in enumerate(index_data[sample["language"]]):
                if existing["file_path"] == sample["file_path"]:
                    index_data[sample["language"]][idx] = sample
                    break

        save_index(index_data)
        print(f"   Progress: {min(i + MAX_WORKERS, len(all_unprocessed))}/{len(all_unprocessed)}")

    print(f"\n✅ Indexing complete!")
    print(f"   Index saved to: {INDEX_FILE}")


if __name__ == "__main__":
    asyncio.run(main())
